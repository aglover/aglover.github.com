<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: elasticsearch | The Disco Blog]]></title>
  <link href="http://thediscoblog.com/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://thediscoblog.com/"/>
  <updated>2015-02-07T11:31:20-08:00</updated>
  <id>http://thediscoblog.com/</id>
  <author>
    <name><![CDATA[Andrew Glover]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HipChat's 1 billion messages served]]></title>
    <link href="http://thediscoblog.com/blog/2014/01/06/hipchats-1-billion-messages-served/"/>
    <updated>2014-01-06T13:50:00-08:00</updated>
    <id>http://thediscoblog.com/blog/2014/01/06/hipchats-1-billion-messages-served</id>
    <content type="html"><![CDATA[<p>There's an <a href="http://blog.hipchat.com/2013/10/16/how-hipchat-scales-to-1-billion-messages/">instructive post on the HipChat blog</a> regarding how their product's message count has risen from 110 million to over a billion since they were acquired by Atlassian. Central to their scaling strategy to meet data growth was their migration from Lucene to <a href="http://thediscoblog.com/blog/categories/elasticsearch/">Elasticsearch</a>.</p>

<!-- more -->


<p>In their words they:</p>

<p><blockquote><p>kicked Lucene to the curb in favor of Elasticsearch.</p><footer><strong>Zuhaib Siddique</strong> <cite><a href='http://blog.hipchat.com/2013/10/16/how-hipchat-scales-to-1-billion-messages/'>How HipChat Scales to 1 Billion Messages</a></cite></footer></blockquote></p>

<p>Their reasoning? <a href="http://thediscoblog.com/blog/2013/09/03/effortless-elasticsearch-clustering/">Elasticsearch's cluttering</a>  allows them to:</p>

<p><blockquote><p>add more nodes to our cluster when we need more capacity, so we can handle extra load while concurrently serving requests. Moreover, the ability to have our shards replicated across the cluster means if we ever lose an instance, we can still continue serving requests, reducing the amount of time HipChat Search is offline.</p><footer><strong>Zuhaib Siddique</strong> <cite><a href='http://blog.hipchat.com/2013/10/16/how-hipchat-scales-to-1-billion-messages/'>How HipChat Scales to 1 Billion Messages</a></cite></footer></blockquote></p>

<p>The other two primary pieces of their architecture haven't changed: CouchDB and <a href="http://thediscoblog.com/blog/categories/redis/">Redis</a>. Interestingly, HipChat <a href="http://www.ibm.com/developerworks/library/j-javadev2-11/">shards</a> Redis data:</p>

<p><blockquote><p>we shard our data over 3 Redis servers, with each server having its own slave.</p><footer><strong>Zuhaib Siddique</strong> <cite><a href='http://blog.hipchat.com/2013/10/16/how-hipchat-scales-to-1-billion-messages/'>How HipChat Scales to 1 Billion Messages</a></cite></footer></blockquote></p>

<p>Sharding in <a href="http://redis.io/">Redis</a> isn't supported out of the box and requires an application to determine how to partition data across shards.  I wonder how HipChat data is partitioned and if their strategy evenly smears data across their shards -- like with any sharing strategy, once you've sharded on a particular key, <a href="http://www.javaworld.com/article/2073449/think-twice-before-sharding.html">it's extremely difficult to re-shard</a>.</p>

<p>"<a href="http://blog.hipchat.com/2013/10/16/how-hipchat-scales-to-1-billion-messages/">How HipChat scales to 1 Billion Messages</a>" is a good read -- so what are you waiting for? Check <a href="http://blog.hipchat.com/2013/10/16/how-hipchat-scales-to-1-billion-messages/">it out</a>!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Just require tire to inquire]]></title>
    <link href="http://thediscoblog.com/blog/2013/12/29/just-require-tire-to-inquire/"/>
    <updated>2013-12-29T12:53:00-08:00</updated>
    <id>http://thediscoblog.com/blog/2013/12/29/just-require-tire-to-inquire</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/mine/es-bonzai.jpg">In the land of <a href="http://thediscoblog.com/blog/categories/ruby/">Ruby</a> there's a few client libraries for <a href="http://thediscoblog.com/blog/categories/elasticsearch/">Elasticsearch</a>, however, one stands above the rest with a comprehensive set of features and remarkable <a href="http://karmi.github.io/retire/">documentation</a>: <a href="https://github.com/karmi/retire">Tire</a>.</p>

<p>With its DSL based API, you'll find working with <a href="http://www.ibm.com/developerworks/library/j-javadev2-24/">Elasticsearch</a> straightforward and a lot of fun. And even though the project's repository has been renamed to <a href="https://github.com/karmi/retire/wiki/Tire-Retire">retire</a>, the Tire library</p>

<p><blockquote><p>will continue to work...bugs will be fixed and important features will be added.</p><footer><strong>Karel Minarik</strong> <cite><a href='https://github.com/karmi/retire/wiki/Tire-Retire'>Tire Retire</a></cite></footer></blockquote></p>

<p>In my opinion, for the time being, Tire is still superior to the <a href="https://github.com/elasticsearch/elasticsearch-ruby">elasticsearch-ruby</a> alternative in terms of features and its elegant DSL.</p>

<!-- more -->


<p></p>

<p>To begin using Tire, grab the gem. For instance, if you're using Bundler, then add the following line to your <code>Gemfile</code>:</p>

<p><code>ruby Adding Tire to a Gemfile
gem 'tire'
</code></p>

<p>After running a <code>bundle install</code>, you'll be able to require Tire.</p>

<p><code>ruby Requiring Tire
require 'tire'
</code></p>

<p>Tire assumes a locally running instance of Elasticsearch; if you wish to connect to a remote node, then create a <code>configure</code> block and set the <code>url</code> to a remote Elasticsearch instance.</p>

<p><code>ruby Configuring a remote Elasticsearch instance
Tire.configure do
  url environment['elasticsearch_host']
end
</code></p>

<p>In the code above, the <code>url</code> is set the value of <code>elasticsearch_host</code>.</p>

<h5>Dealing with Indexes</h5>

<p>First and foremost to working with Elasticsearch is the creation of an index, which can be thought of as a database. Search-able documents are then stored in an index and the process of adding documents to an index is known as <em>indexing</em>.</p>

<p>Consequently, to create an index, you simply name it an issue a <code>create</code> in an <code>index</code> block.</p>

<p><code>ruby Creating an index
Tire.index 'beer_recipes' do
  create
end
</code></p>

<p>In this case, the index is named 'beer_recipes'.</p>

<p>As I've <a href="http://thediscoblog.com/blog/2013/09/14/understanding-elasticsearch-analyzers/">covered before</a>, you can alter how Elasticsearch indexes a document by providing a customized index mapping. In the code below, I've specified that the <code>ingredients</code> property of the <code>beer</code> type will be analyzed using the snowball algorithm, which converts a word into its root, yielding a simpler token (i.e. 'lemons' becomes 'lemon', 'jazzy' becomes 'jazz').</p>

<p>``` ruby Changing the mapping of an index
Tire.index 'beer_recipes' do
  create mappings:  {</p>

<pre><code>beer: {
    properties: { 
        ingredients:  { type: 'string', analyzer: 'snowball' } 
    } 
}
</code></pre>

<p>  }
end
```</p>

<p>Finally, to delete an index, simply issue a <code>delete</code>, like so:</p>

<p>``` ruby Delete an index
Tire.index 'beer_recipes' do</p>

<pre><code>delete
</code></pre>

<p>end
```</p>

<p>Of course, the whole point of <a href="http://thediscoblog.com/blog/2013/05/14/the-democratization-of-search/">Elasticsearch is search</a>! And searching with Tire is super simple; what's more, Tire offers a slick feature of allowing you to define model objects for both indexing and searching (think <a href="https://gist.github.com/karmi/3200212">ActiveRecord</a> integration).</p>

<h5>Indexing &amp; Searching with Tire</h5>

<p>In <a href="http://thediscoblog.com/blog/categories/elasticsearch/">Elasticsearch</a>, an index can be thought of as a database and a <em>type</em> can be thought of as a database table. Accordingly, when you index a document, you give it a type and associate properties, which essentially act like columns.</p>

<p>Indexing a document is executed via the <code>store</code> command. In the code below, a document is stored as a <code>beer</code> type with three properties: <code>name</code>, <code>style</code>, and <code>ingredients</code>. The <code>refresh</code> command updates the index and accordingly makes this newly indexed document available for search.</p>

<p>``` ruby Indexing a document
Tire.index 'beer_recipes' do</p>

<pre><code>store type: 'beer',
name: "Todd Enders' Witbier",
style: "wit, Belgian ale, wheat beer",
ingredients: "4.0 lbs Belgian pils malt, 4.0 lbs raw soft red winter wheat, 0.5 lbs rolled oats, 0.75 oz coriander, freshly ground Zest from two table oranges and two lemons, 1.0 oz 3.1% AA Saaz, 3/4 corn sugar for priming, Hoegaarden strain yeast"
</code></pre>

<p>  refresh
end
```</p>

<p>Consequently, to search for a document in an index, you use the <code>search</code> method, which returns a list of results.</p>

<p>``` ruby Searching for a document
search_res = Tire.search('beer_recipes') do</p>

<pre><code>query do
    term :ingredients, 'lemon'
end
</code></pre>

<p>end</p>

<p>assert_equal 1, search_res.results.size
assert_equal "Todd Enders' Witbier", search_res.results[0].name
```</p>

<p>In the code above, the <code>search</code> block defines a simple query where the <code>ingredients</code> property is searched for the term 'lemon', which yields one result. Note, the resultant list contains maps whose keys are the document's properties (i.e. <code>name</code>, <code>style</code>, and <code>ingredients</code>).</p>

<p>You can use normal Ruby objects for both indexing and searching in Tire. The only requirement is that your model object provide a <code>type</code>, <code>_type</code> or <code>document_type</code> method as well as a <code>to_indexed_json</code> method.</p>

<p>In this case, I've defined a <code>Beer</code> class that includes the two required methods as well as a class method that enables searching the <code>ingredients</code> property.</p>

<p>``` ruby Using a model object
class Beer</p>

<pre><code>attr_reader :name, :style, :ingredients

class &lt;&lt; self
    def search_ingredients_for(ingredient)
        search_res = Tire.search('beer_recipes') do
        query do
            term :ingredients, ingredient
        end
    end
    search_res.results
    end
end

def initialize(attributes={})
    @attributes = attributes
@attributes.each_pair { |name,value| instance_variable_set :"@#{name}", value }
</code></pre>

<p>  end</p>

<p>  def type</p>

<pre><code>  'beer'
</code></pre>

<p>  end</p>

<p>  def to_indexed_json</p>

<pre><code>@attributes.to_json
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>Next, to make use of my <code>Beer</code> model object, I need to configure Tire to use it via the <code>wrapper</code> command. Once that is done, I can use my <code>Beer</code> object to index beers and to search for them.</p>

<p>``` ruby
Tire.configure do</p>

<pre><code>wrapper Beer #forces results to be of type beer!
</code></pre>

<p>end</p>

<p>Tire.index 'beer_recipes' do</p>

<pre><code>store Beer.new type: 'beer',
name: "Todd Enders' Witbier",
style: "wit, Belgian ale, wheat beer",
ingredients: "4.0 lbs Belgian pils malt, 4.0 lbs raw soft red winter wheat, 0.5 lbs rolled oats, 0.75 oz coriander, freshly ground Zest from two table oranges and two lemons, 1.0 oz 3.1% AA Saaz, 3/4 corn sugar for priming, Hoegaarden strain yeast"
</code></pre>

<p>  refresh
end</p>

<p>results = Beer.search_ingredients_for 'lemons'
assert_equal 1, results.size
assert_equal Beer, results[0].class
assert_equal "Todd Enders' Witbier", results[0].name
```</p>

<p>As you can see above, the <code>store</code> method takes a new instantiated <code>Beer</code> instance; what's more, I can use my <code>search_ingredients_for</code> class method to find a corresponding document.</p>

<p>While there are alternate Ruby <a href="http://thediscoblog.com/blog/2013/01/02/scalable-searching-with-elasticsearch/">Elasticsearch</a> libraries available, Tire is by far the richest -- its features along with its DSL make working with <a href="http://www.ibm.com/developerworks/library/j-javadev2-24/">Elasticsearch</a> easy along with fun. So what are you waiting for? Require Tire to inquire!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch in a box]]></title>
    <link href="http://thediscoblog.com/blog/2013/11/25/elasticsearch-in-a-box/"/>
    <updated>2013-11-25T13:42:00-08:00</updated>
    <id>http://thediscoblog.com/blog/2013/11/25/elasticsearch-in-a-box</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/mine/esbox.jpg">Are you looking to get going with <a href="http://thediscoblog.com/blog/categories/elasticsearch/">Elasticsearch</a> as quickly as possible without having to worry about <a href="http://thediscoblog.com/blog/2013/05/17/elasticsearch-on-ec2-in-less-than-60-seconds/">installing Java or Elasticsearch</a> itself? Are you looking for a repeatable and automated mechanism for bringing up Elasticsearch instances for developmental and or testing purposes? While there's certainly a number of <a href="http://www.elasticsearch.org/">Elasticsearch</a>-as-a-platform service providers out there, there's one other option: use <a href="https://github.com/aglover/coffer">Elasticsearch-in-a-box</a>.</p>

<p>Elasticsearch-in-a-box is a freely available <a href="http://www.vagrantbox.es/">Vagrant base box</a>. What that means is that you can quickly fire up and tear down an Elasticsearch environment with <a href="http://docs.vagrantup.com/v2/getting-started/">simple commands</a> like <code>vagrant up</code> and <code>vagrant destroy</code>.</p>

<!-- more -->


<p>In order to use Elasticsearch-in-a-box, you first need to have <a href="http://docs.vagrantup.com/v2/installation/">Vagrant</a> and <a href="https://www.virtualbox.org/">VirtualBox</a> installed. These two installations couldn't be any easier. To install <a href="http://thediscoblog.com/blog/2013/10/16/ssh-and-vagrant/">Vagrant</a>, simply go to the downloads page and pick your target distribution. Vagrant provisions machines on top of virtual machine providers like VMWare, <a href="http://thediscoblog.com/blog/categories/aws/">AWS</a>, and VirtualBox. VirtualBox is free and easy to install -- like Vagrant, simply <a href="https://www.virtualbox.org/wiki/Downloads">go to the downloads section</a> and pick your target platform.</p>

<p>Once you have both Vagrant and VirtualBox installed, you are two steps away from Elasticsearch-ing.</p>

<p>First, you need to add and initialize the Elasticsearch-in-a-box <a href="http://docs.vagrantup.com/v2/boxes.html">template</a>. Go ahead and create a directory, like <code>/projects/esinabox</code>, change directories into it and execute this command:</p>

<p><code>bash This command will create a Vagrant definition named esinabox from the downloaded template
vagrant box add esinabox https://s3.amazonaws.com/coffers/esinabox.box
</code></p>

<p>This command will download the Elasticsearch-in-a-box template. Once that completes (it'll take a few moments depending on your connection), execute this command:</p>

<p><code>bash Vagrant init will create a VagrantFile
vagrant init 'esinabox'
</code></p>

<p>This command will create a <code>VagrantFile</code>, which you can use to customize the Elasticsearch-in-a-box instance. By default, you shouldn't need to do much, however, you can map network ports, install additional software via <a href="http://thediscoblog.com/blog/2013/11/18/provisioning-ubuntu-with-java-in-3-steps/">Bash</a>, Chef, and Puppet at your discretion.</p>

<p>Next, fire up Elasticsearch-in-a-box like so:</p>

<p><code>bash Starting up Elasticsearch-in-a-box
vagrant up
</code></p>

<p>Now that Elasticsearch-in-a-box is running locally on your machine, you can open up a new terminal and execute RESTful commands like normal because Elasticsearch is running on same ports: 9200 &amp; 9300. So go ahead and execute some queries, like so:</p>

<p><code>bash Elasticsearch is up an running!
curl -XGET 'http://localhost:9200/_status?pretty=true'
</code></p>

<p>And when you are done, go ahead and tear down the instance like so:</p>

<p><code>bash Destroying a VM instance
vagrant destroy -f
</code></p>

<p>Wasn't that easy? The Elasticsearch-in-a-box Vagrant template was built using <a href="https://github.com/jedi4ever/veewee">Veewee</a>. The base box is 64-bit <a href="http://www.ubuntu.com/index_asus.html">Ubuntu 12.04</a> with Oracle's <a href="http://thediscoblog.com/blog/2013/11/18/provisioning-ubuntu-with-java-in-3-steps/">Java 7</a> and <a href="http://www.elasticsearch.org/download/">Elasticsearch version 0.90.7</a>.</p>

<p>If you're looking for a quick and easy way to automatically provision Elasticsearch, then look no further and give <a href="https://github.com/aglover/coffer">Elasticsearch-in-a-box</a> a try!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I like my ElasticSearch a la Node.js]]></title>
    <link href="http://thediscoblog.com/blog/2013/09/21/i-like-my-elasticsearch-a-la-node-dot-js/"/>
    <updated>2013-09-21T08:57:00-07:00</updated>
    <id>http://thediscoblog.com/blog/2013/09/21/i-like-my-elasticsearch-a-la-node-dot-js</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/mine/320px-Pie_A_La_Mode.jpg">While <a href="http://thediscoblog.com/blog/categories/elasticsearch/">ElasticSearch</a> is easy enough to work with via its RESTful HTTP API, there are <a href="http://www.elasticsearch.org/guide/clients/">myriad client libraries</a> available in almost every conceivable programming language. If <a href="http://thediscoblog.com/blog/categories/node/">Node.js</a> is your language of choice, then there's at least two actively supported libraries available.</p>

<p>My favorite is dubbed, albeit rather dully, "<a href="https://github.com/phillro/node-elasticsearch-client">Elastic Search Client</a>", but don't let the library's unimaginative name fool you: this is a handy library that allows you to do everything you could do via cURL with the added benefit of JavaScript callbacks. Best of all, you can use the Node Elastic Search Client in <a href="http://thediscoblog.com/blog/categories/coffeescript/">Coffeescript</a>, which is a handy language that makes JavaScript less verbose and that ultimately compiles into JavaScript.</p>

<!-- more -->


<p>Accordingly, if you're familiar with the typical RESTful API calls for creating and mapping indexes, plus indexing and searching documents, then you'll find Elastic Search Client easy enough to pick up.</p>

<p>To get started, add the library as a dependency in your NPM <code>package.json</code> file like so:</p>

<p>``` json My package.json file that lists the latest version of elasticsearchclient.
"dependencies":{</p>

<pre><code>"elasticsearchclient" : "latest",
"mocha" : "latest",
"should" : "latest",
"coffee-script" : "latest"
</code></pre>

<p>}
```</p>

<p>Since I happen to prefer <a href="http://coffeescript.org/">CoffeeScript</a> over JavaScript, I've also included CoffeeScript as a dependency.</p>

<p>Like any Node library, you'll need to include a library it via a <code>require</code> statement to make use of it. In this case, I'll require <code>'elasticsearchclient'</code> and then connect to a local instance like so:</p>

<p><code>javascript Initalizing a new client
ElasticSearchClient = require 'elasticsearchclient'
client = new ElasticSearchClient { host: 'localhost', port: 9200 }
</code></p>

<p>Going forward, I'm going to reference a few variables, namely <code>indexName</code>, which is "beer_recipes" and <code>objName</code>, which is "beer". What's more, this code is using <a href="http://visionmedia.github.io/mocha/">Mocha</a> and <a href="https://github.com/visionmedia/should.js/">should</a>, so that'll explain the various specification related statements in the examples below.</p>

<p>With a connection to an Elasticsearch server, I can consequently create an index like so:</p>

<p>``` javascript Creating an index in a before clause
describe 'Create and update an index', -></p>

<pre><code>before (done) -&gt;
    client.createIndex(indexName).on 'data', (data) -&gt;
        data = JSON.parse data
        data.should.be.ok
        done()
    .exec()
</code></pre>

<p>```</p>

<p>And I can update the index's mapping too. Just use the <code>putMapping</code> call:</p>

<p>``` javascript Updating an Index mapping
it 'should support a mapping put which changes the analyzer', (done) -></p>

<pre><code>snowball = { "mappings" : { "beer" : { "properties" : { "ingredients" : { "type" : "string", "analyzer" : "snowball" } } } }}
client.putMapping(indexName, objName, snowball).on 'data', (data) -&gt;
    data = JSON.parse data
    data.should.be.ok
    done()
.exec()
</code></pre>

<p>```</p>

<p>In this case, the actual mapping JSON document is identical to what I'd have to pass via cURL, for instance.</p>

<p>You should start to notice a pattern with respect to how the Elastic Search Client deals with callbacks -- using an <code>on</code> method, you can register a callback for <code>'data'</code>, which essentially entails the response from the server; moreover, you can also register a listener for <code>'error'</code>, which as you can imagine, gets invoked if there is a problem.</p>

<p>Deleting an index is just as easy as creating one too:</p>

<p>``` javascript Deleting an index
after (done) -></p>

<pre><code>client.deleteIndex(indexName).on 'data', (data) -&gt;
    data = JSON.parse data
    data.should.be.ok
    done()
.exec()
</code></pre>

<p>```</p>

<p>To index a document, you simply use the <code>index</code> function like so:</p>

<p>``` javascript Indexing a document
beer = { "name": "Todd Enders' Witbier", "style": "wit, Belgian ale, wheat beer", "ingredients": "4.0 ..."}
client.index(indexName, objName, beer).on 'data', (data) -></p>

<pre><code>data = JSON.parse data
data.ok.should.equal true
</code></pre>

<p>.exec()
```</p>

<p>Note in this case, I pass along an index name, and object name, and a JSON document to index. Because JSON marries so nicely with JavaScript, this library is quite nice, don't you think?</p>

<p>Searching is just as easy (as I'm sure you already guessed). You create a query JSON document and pass it along to the <code>search</code> method like so:</p>

<p>``` javascript Searching is just as easy!
iquery = { "query" : { "term" : { "ingredients" : "lemons" } } }
client.search(indexName, objName, iquery).on 'data', (idata) -></p>

<pre><code>result = JSON.parse idata
result.hits.total.should.equal 1
done()
</code></pre>

<p>.exec()
```</p>

<p>The resultant response from the server is a JSON document that must be parsed and dealt with accordingly.</p>

<p>Of course, you can grab an individual document via its id just as easily:</p>

<p>``` javascript Getting an individual document via its document id
client.get(indexName, objName, doc_id).on 'data', (getData) -></p>

<pre><code>doc = JSON.parse getData
doc._source.name.should.equal "Todd Enders' Witbier"
done()
</code></pre>

<p>.exec()
```</p>

<p>Node's Elastic Search Client is a breeze to pick up; plus, the natural union of JSON and JavaScript make Node a perfect language for interfacing with ElasticSearch. Dig it?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding ElasticSearch analyzers]]></title>
    <link href="http://thediscoblog.com/blog/2013/09/14/understanding-elasticsearch-analyzers/"/>
    <updated>2013-09-14T08:37:00-07:00</updated>
    <id>http://thediscoblog.com/blog/2013/09/14/understanding-elasticsearch-analyzers</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/mine/old-beer.jpg">Sadly, lots of early Internet beer recipes aren't necessarily in an easily digestible format; that is, these recipes are <em>unstructured</em> intermixed lists of directions and ingredients often originally composed in an email or forum post.</p>

<p>So while it's hard to easily put these recipes into traditional data stores (ostensibly for easier searching), they're perfect for <a href="http://thediscoblog.com/blog/2013/05/14/the-democratization-of-search/">ElasticSearch</a> in their current form.</p>

<p>Accordingly, imagine an <a href="http://thediscoblog.com/blog/2013/05/17/elasticsearch-on-ec2-in-less-than-60-seconds/">ElasticSearch</a> index full of beer recipes, since...well...I enjoy making beer (and drinking it too).</p>

<!-- more -->


<p>First, I'll add some beer recipes into ElasticSearch using <a href="https://github.com/phillro/node-elasticsearch-client">Node's ElasticSearch Client</a>(note that the code is <a href="http://thediscoblog.com/blog/categories/coffeescript/">CoffeeScript</a> though). I'll be adding these beer recipes into a <code>beer_recipes</code> index like so:</p>

<p>``` javascript Adding a beer recipe
beer_1 = {</p>

<pre><code>name: "Todd Enders' Witbier",
style: "wit, Belgian ale, wheat beer",
ingredients: "4.0 lbs Belgian pils malt, 4.0 lbs raw soft red winter wheat, 0.5 lbs rolled oats, 0.75 oz coriander, freshly ground Zest from two table oranges and two lemons, 1.0 oz 3.1% AA Saaz, 3/4 corn sugar for priming, Hoegaarden strain yeast"
</code></pre>

<p>}</p>

<p>client.index('beer_recipes', 'beer', beer_1).on('data', (data) -></p>

<pre><code>console.log(data)
</code></pre>

<p>).exec()
```</p>

<p>Note how the interesting part of a recipe JSON document, dubbed <code>beer_1</code> is found in the <code>ingredients</code> field. This field is basically a big string of valuable text (you can imagine how this string was essentially the body of an email). So while the <code>ingredients</code> field is unstructured, it's something clearly that people will want to search on.</p>

<p>I will add one more recipe for good measure:</p>

<p>``` javascript Adding a second beer recipe
beer_2 = {</p>

<pre><code>name: "Wit",
style: "wit, Belgian ale, wheat beer",
ingredients: "4 lbs DeWulf-Cosyns 'Pils' malt, 3 lbs brewers' flaked wheat (inauthentic; will try raw wheat nest time), 6 oz rolled oats, 1 oz Saaz hops (3.3% AA), 0.75 oz bitter (Curacao) orange peel quarters (dried), 1 oz sweet orange peel (dried), 0.75 oz coriander (cracked), 0.75 oz anise seed, one small pinch cumin, 0.75 cup corn sugar (priming), 10 ml 88% food-grade lactic acid (at bottling), BrewTek 'Belgian Wheat' yeast"
</code></pre>

<p>}</p>

<p>client.index('beer_recipes', 'beer', beer_2).on('data', (data) -></p>

<pre><code>console.log(data)
</code></pre>

<p>).exec()
```</p>

<p>It's a hot summers day and I'm thinking I'd like to make a beer with <em>lemon</em> as an ingredient (to be clear: I want to use lemon zest, which is obtained from a lemon peel). So naturally, I need to find (i.e. <em>search for</em>) a recipe with lemons in it.</p>

<p>Consequently, I'll search my index for recipes that contain the word "lemon" like so:</p>

<p>``` javascript Searching for lemon
query = { "query" : { "term" : { "ingredients" : "lemon" } } }</p>

<p>client.search('beer_recipes', 'beer', query).on('data', (data) -></p>

<pre><code>data = JSON.parse(data)
for doc in data.hits.hits
    console.log doc._source.style
    console.log doc._source.name
    console.log doc._source.ingredients
</code></pre>

<p>).exec()
```</p>

<p>But nothing shows up -- there are no results! Why is that?</p>

<p>If you look closely in the earlier code example (specifically, the <code>beer_1</code> JSON document), you can see that the word "lemons" is in the text (i.e. "...two table oranges and two lemons..."). It turns out, by default, the way values are indexed by ElasticSearch, <em>lemon</em> doesn't necessarily match -- <em>lemons</em> does though.</p>

<p>``` javascript Searching for lemons
query = { "query" : { "term" : { "ingredients" : "lemons" } } }</p>

<p>client.search('beer_recipes', 'beer', query).on('data', (data) -></p>

<pre><code>data = JSON.parse(data)
for doc in data.hits.hits
    console.log doc._source.style
    console.log doc._source.name
    console.log doc._source.ingredients
</code></pre>

<p>).exec()
```</p>

<p>Lo and behold, this search returns a hit! But that's inconvenient, to say the least. Basically the words in the <code>ingredients</code> field are tokenized <em>as is</em>. Hence, a search for "lemons" works while "lemon" doesn't. Note: there are various mechanisms for searching, and a search on "lemon*" should have returned a result.</p>

<p>When a document is added into an <a href="http://thediscoblog.com/blog/2013/01/02/scalable-searching-with-elasticsearch/">ElasticSearch</a> index, its fields are analyzed and converted into <em>tokens</em>. When you execute a search against an index, you search against those tokens. How ElasticSearch tokenizes a document is configurable.</p>

<p>There are different ElasticSearch analyzers available -- from language analyzers that allow you to support non-English language searches to the <a href="http://snowball.tartarus.org/">snowball</a> analyzer, which converts a word into its root (or stem and that process of creating a stem from a word is called <em><a href="http://snowball.tartarus.org/algorithms/english/stemmer.html">stemming</a></em>), yielding a simpler token. For example, a snowball of "lemons" would be "lemon". Or if the words "knocks" and "knocking" were in a snowball analyzed document, both terms would have "knock" as a stem.</p>

<p>You can change how documents are tokenized via the index mapping API like so:</p>

<p>``` bash Changing the mapping for an index using cURL
curl -XPUT 'http://localhost:9200/beer_recipes' -d '{ "mappings" : {
  "beer" : {</p>

<pre><code>"properties" : {
  "ingredients" : { "type" : "string", "analyzer" : "snowball" }
}
</code></pre>

<p>   }
 }
}'
```</p>

<p>Note how the above mapping specifies that the <code>ingredients</code> field will be analyzed via the snowball analyzer. Also note, you have to change the mapping of an index <em>before</em> you begin to add documents to it! So, in this case, I'll need to drop the index, run the mapping call above, and then re-add those two recipes.</p>

<p>Now I can begin searching recipes for the ingredient "lemon"  or "lemons".</p>

<p>``` javascript Searching for lemon now works!
query = { "query" : { "term" : { "ingredients" : "lemon" } } }</p>

<p>client.search('beer_recipes', 'beer', query).on('data', (data) -></p>

<pre><code>data = JSON.parse(data)
for doc in data.hits.hits
    console.log doc._source.style
    console.log doc._source.name
    console.log doc._source.ingredients
</code></pre>

<p>).exec()
```</p>

<p>Keep in mind that <a href="http://stackoverflow.com/questions/3875382/lucene-standard-analyzer-vs-snowball">snowballing can inadvertently</a> make your search results <em>less relevant</em>. Long words can be stemmed into more common but completely different words. For example, if you snowball a document that contains the word "sextant", the word "sex" will result as a stem. Thus, searches for "sextant" will also return documents that contain the word "sex" (and vice versa).</p>

<p>ElasticSearch puts a powerful search engine into your clutches; plus, with a little forethought into how a document's contents are analyzed, you'll make searches event more relevant.</p>
]]></content>
  </entry>
  
</feed>
