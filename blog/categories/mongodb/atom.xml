<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MongoDB | The Disco Blog]]></title>
  <link href="http://thediscoblog.com/blog/categories/mongodb/atom.xml" rel="self"/>
  <link href="http://thediscoblog.com/"/>
  <updated>2013-09-09T18:40:26-04:00</updated>
  <id>http://thediscoblog.com/</id>
  <author>
    <name><![CDATA[Andrew Glover]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MongoDB pro tip: field projections]]></title>
    <link href="http://thediscoblog.com/blog/2013/07/15/mongodb-pro-tip-field-projections/"/>
    <updated>2013-07-15T15:07:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/07/15/mongodb-pro-tip-field-projections</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/mine/mongodb_icon.png">Did you ever learn that <code>select * from table</code> in RDBMS-land is bad? Of course, you did! If you're only looking for the email address of a user and not the other 15 columns worth of data, then why ask for that data and incur a penalty? The query <code>select email from user where user_id = 1;</code> is far more efficient for the database and the corresponding application that issued it, because there is <em>less data to fetch and consume</em>.</p>

<!-- more -->


<p>As it turns out, the same rule of thumb is true in <a href="http://www.mongodb.org/">MongoDB-land</a>. That is, <code>db.users.find({user_id:1})</code> is just as inefficient as the <code>select *</code> query if all you want is the user's email address. With MongoDB, you can specify a <a href="http://docs.mongodb.org/manual/reference/method/db.collection.find/">projection</a> as a part of your query that ultimately can limit what fields come back.</p>

<p>Thus, if I only want the <code>email</code> field on a user, I can issue a query like so:</p>

<p><code>javascript MongoDB field projection
db.users.find({user_id:1}, {email:1})
</code></p>

<p>The second clause specifies that you only want the <code>email</code> field returned. You can also negate fields by issuing a 0, which means false. To negate <code>first_name</code> and <code>last_name</code>, you would type:</p>

<p><code>javascript MongoDB field negation with 0 or false
db.users.find({user_id:1}, {first_name:0, last_name:0})
</code></p>

<p>In this case, I'd get all fields on that user document <em>except</em> <code>first_name</code> and <code>last_name</code>. Note, you cannot issue both an include and exclude in the same statement.</p>

<p>For you <a href="http://mongoid.org/en/mongoid/index.html">Mongoid</a> users, including specific fields translates to <code>only</code> and excluding them translates to <code>without</code> -- each clause can be attached to a criteria (but not both at the same time). For example, if <code>User</code> is a Mongoid document and I only want an underlying query to grab the <code>email</code> field, then the corresponding Mongoid query would be:</p>

<p><code>ruby Mongoid field projection
User.where(user_id:1).only(:email)
</code></p>

<p>Field projection reduces document sizes on a fetch -- this decreases memory consumption (for example, in the case of Mongoid your models aren't fully populated with data) as well as bandwidth (that is, document retrieval is faster). Both MongoDB and the calling application benefit from field projection -- it's a win-win all the way around.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB primary keys are your friend]]></title>
    <link href="http://thediscoblog.com/blog/2013/06/22/mongodb-primary-keys-are-your-friend/"/>
    <updated>2013-06-22T15:00:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/06/22/mongodb-primary-keys-are-your-friend</id>
    <content type="html"><![CDATA[<p>All documents in a <a href="http://www.mongodb.org/">MongoDB</a> collection have a primary key dubbed <code>_id</code>. This field is automatically assigned to a document upon insert, so there's rarely a need to provide it. What's interesting about the <code>_id</code> field is that it is <em>time based</em>. That is, the underlying type of <code>_id</code>, which is <code>ObjectId</code>, is a <a href="http://docs.mongodb.org/manual/reference/object-id/">12-byte BSON type</a>, and 4 of those bytes represent the seconds since Unix epoch.</p>

<!-- more -->


<p></p>

<p>What's also special about the <code>_id</code> field is that it is automatically indexed as you can see below by calling <code>getIndexes</code> on any collection.</p>

<p>``` javascript All MongoDB collections have an _id field as an index</p>

<blockquote><p>db.things.getIndexes()
[</p>

<pre><code> {
      "v" : 1,
      "key" : {
           "_id" : 1
      },
      "ns" : "test.things",
      "name" : "_id_"
 }
</code></pre>

<p>]
```</p></blockquote>

<p>And as everyone remembers from traditional RDBMSs, <a href="http://en.wikipedia.org/wiki/Database_index">indexes are important</a> because they can make document retrieval faster; nevertheless, indexes do consume memory and there is a slight performance penalty when inserting documents as all corresponding indexes must be updated. Thus, while you should seriously consider using indexes, you need to be economical in their usage.</p>

<p>Naturally, searching by a document's <code>_id</code> is only convenient when you <em>know</em> it. More often than not, documents are searched via other fields and if you find yourself searching <a href="http://cookbook.mongodb.org/patterns/date_range/">via a time series</a>, such as <code>created_at</code> then you are in for a treat.</p>

<p>Imagine a collection dubbed <code>logs</code> that contains simple documents capturing various log messages. A sample document could look like so:</p>

<p>``` javascript A simple document in a logs collection
{</p>

<pre><code> "_id" : ObjectId("51c4ab6d4d6906d494460728"),
 "message" : "crashed, no such method exception",
 "type" : "crash",
 "created_at" : ISODate("2013-06-21T19:37:17.992Z")
</code></pre>

<p>}
```</p>

<p>What if I wanted to find all log messages for some date, like today? I could write my query like so:</p>

<p><code>javascript finding all logs created since June 20th, 2013
db.logs.find({created_at:{'$gt': new Date(2013, 5, 20)}})
</code></p>

<p>If I throw an explain to that query, I can see that because I do not have an index on <code>created_at</code>, a basic cursor is leveraged and all documents in the collection were scanned in order to retrieve my result.</p>

<p>``` javascript An explain plan attached to my find</p>

<blockquote><p>db.logs.find({created_at:{'$gt': new Date(2013, 5, 20)}}).explain()
{</p>

<pre><code> "cursor" : "BasicCursor",
 "isMultiKey" : false,
 "n" : 2,
 "nscannedObjects" : 4,
 "nscanned" : 4,
 "nscannedObjectsAllPlans" : 4,
 "nscannedAllPlans" : 4,
 "scanAndOrder" : false,
 "indexOnly" : false,
 "nYields" : 0,
 "nChunkSkips" : 0,
 "millis" : 0,
 "indexBounds" : {
</code></pre></blockquote>

<pre><code> },
 "server" : "ghome-computer.home:27017"
</code></pre>

<p>}
```</p>

<p>As you can see, searching via the <code>created_at</code> field can be inefficient; thus, you might be tempted to throw an index on that field. This would naturally make that particular query more efficient, however, you would incur the cost of a new index which is more memory consumed and inserts would be slightly slower due to an update to that newly created index.</p>

<p>As it turns out, because the <code>_id</code> field embeds Unix epoch in it, you can just as easily craft a find expression <em>without</em> including the <code>created_at</code> field. For example, the <a href="https://github.com/mongodb/mongo-ruby-driver">MongoDB Ruby driver</a> allows you to create <code>ObjectId</code>'s from a <code>Time</code> like so:</p>

<p><code>ruby Creating a new ObjectId via the from_time factory method
yesterday = Time.now - (60*60*(24*1))
custom_id = BSON::ObjectId.from_time(yesterday)
=&gt; BSON::ObjectId('51c397800000000000000000')
</code></p>

<p>As you can see, I've created a new <code>ObjectId</code> via the <code>from_time</code> factory method.  51c397800000000000000000 is a hexadecimal representation and the first 8 digits represent the time with everything else zeroed out. Note, you can create <code>ObjectId</code> types via some notion of time in other language drivers as well.</p>

<p>Now I can leverage my <code>custom_id</code> in any <code>find</code> expression. Via the Ruby driver, I can also attach an <code>explain</code>, which'll demonstrate the usage of the free <code>_id</code> index.</p>

<p>``` ruby Using a date derived ObjectId forces a find to use the <em>id index
 mongodb[:logs].find({</em>id: {'$gt' => custom_id}}).explain</p>

<p>=> {"cursor"=>"BtreeCursor <em>id</em>", "isMultiKey"=>false, "n"=>1, "nscannedObjects"=>1, "nscanned"=>1, ....}
```</p>

<p>If you see <code>BtreeCusor</code>, then you know you're using an index; if you see <code>BasicCursor</code>, you know you're not.</p>

<p>Thus, if you find yourself executing queries and creating indexes for some time or date field like <code>created_at</code>, you might be better off just using Mongo's <code>_id</code> field as it already embeds the notion of <em>created at</em> and is indexed by default. Dig it?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Backgrounding tasks in Heroku with Delayed Job]]></title>
    <link href="http://thediscoblog.com/blog/2013/06/10/backgrounding-tasks-in-heroku-with-delayed-job/"/>
    <updated>2013-06-10T12:56:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/06/10/backgrounding-tasks-in-heroku-with-delayed-job</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/mine/heroku-logo-light.png">Long running web requests are bad. They create resource contention among other waiting incoming requests; what's more, with HTTP servers like <a href="https://github.com/defunkt/unicorn">Unicorn</a> (which is rapidly becoming the de facto HTTP server for Rails apps running on <a href="https://get.heroku.com/">Heroku</a>), long running requests are summarily terminated after some threshold (for example, 30 seconds) leaving users with a nasty timeout error (and, of course, other users complaining that it takes forever for your site to do anything). Luckily, with frameworks like <a href="https://github.com/collectiveidea/delayed_job">Delayed Job</a>, backgrounding long running tasks <a href="https://devcenter.heroku.com/articles/delayed-job">couldn't be any easier</a>.</p>

<!-- more -->


<p>To leverage Delayed Job in Heroku, you'll need for follow a few steps. In my case, since I'm using <a href="http://mongoid.org/en/mongoid/index.html">Mongoid</a>, I'm going to use an <a href="https://github.com/collectiveidea/delayed_job_mongoid">alternate Active Record Delayed Job tie-in</a> dubbed <code>delayed_job_mongoid</code>. The reason for this <a href="http://guides.rubyonrails.org/">Active Record</a> integration will become obvious once I show you how to actually force a long running task to be asynchronous. Regardless if you are using Mongoid or not, the Active Record integration is identical.</p>

<p>First and foremost, you'll need to add <code>delayed_job_mongoid</code> to your <code>Gemfile</code> (or if you aren't using Mongoid, add <code>delayed_job_active_record</code>).  Delayed Job stores jobs in your database; consequently, after you run <code>bundle install</code>, proceed to update your underlying MongoDB instance with a new index by running:</p>

<p><code>bash Adding an index and new collection to MongoDB
$&gt; script/rails runner 'Delayed::Backend::Mongoid::Job.create_indexes'
</code></p>

<p>Obviously, if you're using a normal RDMBS, your command will be slightly different (i.e. <code>rake db:migrate</code>).</p>

<p>After that step is complete, you'll need to generate a <code>delayed_job</code> script via:</p>

<p><code>bash Generating the Delayed Job script
$&gt; rails generate delayed_job
</code></p>

<p>This script allows to you execute the Delayed Job framework with a number of options. As it turns out, you don't really need this script as Delayed Job hooks into Rake as well; it is via the Rake integration that Heroku will work with Delayed Job.</p>

<p>Now that you've set up 80% of the infrastructure required to support Delayed Job, you next need to background some long running task. This is the fun part and like everything else so far, it couldn't be any easier.</p>

<p>Delayed Job hooks directly into Active Record and thus, you can background some unit of work on a model object by inserting a <code>delay</code> call. For example, in my case, I had a <code>Group</code> model object that when updated under certain circumstances required that <em>all associated</em> users also be updated (wouldn't it be nice if <a href="https://jira.mongodb.org/browse/SERVER-124">MongoDB supported triggers</a>?). As you can imagine, updating hundreds of users is rather time intensive.</p>

<p>Thus, the user update logic was thrown into a special method (dubbed <code>aysnc_update_users</code>) and was then invoked like so:</p>

<p><code>ruby Adding in a delay backgrounds the async_update_users call
group.delay.async_update_users
</code></p>

<p>This call essentially serializes the logic here into a collection called <code>delayed_backend_mongoid_jobs</code> which can then be invoked asynchronously by another process. And therein lies your last few steps.</p>

<p>Heroku has the notion of <a href="https://devcenter.heroku.com/articles/background-jobs-queueing">worker dynos</a>, which are background processors -- think of worker dynos as engines capable of doing what ever you want them to do. In the case of Delayed Job, you want those worker dynos to invoke jobs residing in the <code>delayed_backend_mongoid_jobs</code> collection (which is conceptually a queue).</p>

<p>To configure a worker dyno, you'll need to do 2 things. First, provision one like so:</p>

<p><code>bash Using Heroku CLI to fire up a worker dyno
$&gt; heroku ps:scale worker=1
</code></p>

<p>You'll then need to update your <code>Procfile</code> to include the command the worker dyno should invoke:</p>

<p><code>ruby Updated Procfile command
worker:  bundle exec rake jobs:work
</code></p>

<p>As you can see, the worker dyno in this case is invoking a <a href="http://rake.rubyforge.org/">Rake task</a> (<code>jobs:work</code>) which is asynchronously polling for new jobs in your conceptual queue and invoking them as they are found.</p>

<p>Finally, deploy to Heroku and keep an eye on the queue for jobs -- you can isolate the dyno workers logs via</p>

<p><code>
$&gt; heroku logs -p worker -t
</code></p>

<p>Was that easy or what?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB to CSV]]></title>
    <link href="http://thediscoblog.com/blog/2013/06/07/mongodb-to-csv/"/>
    <updated>2013-06-07T20:01:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/06/07/mongodb-to-csv</id>
    <content type="html"><![CDATA[<p>Every once in a while, I need to give a non-technical user (like a business analyst) data residing in <a href="http://www.mongodb.org/">MongoDB</a>; consequently, I export the target data as a <a href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV file</a> (which they can presumably slice and dice once they import it into Excel or some similar tool). Mongo has a handy <a href="http://docs.mongodb.org/manual/reference/program/mongoexport/">export utility</a> that takes a bevy of options, however, there is an <a href="https://jira.mongodb.org/browse/SERVER-4224">outstanding bug</a> and some <a href="http://stackoverflow.com/questions/6814151/how-to-export-collection-to-csv-in-mongodb">general confusion</a> as to how to properly export data in CSV format.</p>

<!-- more -->


<p>Accordingly, if you need to export some specific data from MongoDB into CSV format, here's how you do it. The key parameters are connection information to include authentication, an output file, and most important, a list of fields to export. What's more, you can provide a query in escaped JSON format.</p>

<p>You can find the <code>mongoexport</code> utility in your Mongo installation <code>bin</code> directory. I tend to favor verbose parameter names and explicit connection information (i.e. rather than a URL syntax, I prefer to spell out the host, port, db, etc directly).  As I'm targeting specific data, I'm going to specify the collection; what's more, I'm going to further filter the data via a query.</p>

<p><code>ObjectId</code>'s can be referenced via the <code>$oid</code> format; furthermore, you'll need to escape all JSON quotes. For example, if my query is against a <code>users</code> collection and filtered by <code>account_id</code> (which is an <code>ObjectId</code>), the query via the <code>mongo</code> shell would be:</p>

<p><code>javascript Mongo Shell Query
db.users.find({account_id:ObjectId('5058ca07b7628c0002099006')})
</code></p>

<p>Via the command line &agrave; la <code>monogexport</code>, this translates to:</p>

<p><code>bash Collections and queries
 --collection users --query "{\"account_id\": {\"\$oid\": \"5058ca07b7628c0002000006\"}}"
</code></p>

<p>Finally, if you want to only export a portion of the fields in a <code>user</code> document, for example, <code>name</code>, <code>email</code>, and <code>created_at</code>, you need to provide them via the <code>fields</code> parameter like so:</p>

<p><code>bash Fields declaration
--fields name,email,created_at
</code></p>

<p>Putting it all together yields the following command:</p>

<p><code>bash Puttin' it all together
mongoexport --host mgo.acme.com --port 10332 --username acmeman --password 12345  \
--collection users --csv --fields name,email,created_at --out all_users.csv --db my_db \
--query "{\"account_id\": {\"\$oid\": \"5058ca07b7628c0999000006\"}}"
</code></p>

<p>Of course, you can throw this into a bash script and parameterize the <code>collection</code>, <code>fields</code>, output file, and query with bash's handy <code>$1</code>, <code>$2</code>, etc variables.</p>

<p>Got it?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mongoid batch inserts]]></title>
    <link href="http://thediscoblog.com/blog/2013/03/27/mongoid-batch-inserts/"/>
    <updated>2013-03-27T11:34:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/03/27/mongoid-batch-inserts</id>
    <content type="html"><![CDATA[<p>In SQL land, all databases support batch inserts. Batch inserts are an effective and efficient mechanism to insert a lot of similar data. That is, instead of issuing x insert statements, you execute 1 insert with x records. This is much more efficient because the insert statement doesn't need to be re-parsed x times, there is only 1 network trip as opposed to x, and in the case of transactions, there is only 1 transaction instead of x. When compared to x inserts, batch inserts are always faster.</p>

<p>As it turns out, <a href="http://www.mongodb.org/">MongoDB</a> supports <a href="http://docs.mongodb.org/manual/applications/create/#bulk-insert-multiple-documents">batch inserts</a>! And just like in SQL land, Mongo's batching feature is much faster at inserting a lot of data in one insert rather than x inserts.</p>

<p>For example, the <a href="https://github.com/mongodb/mongo-ruby-driver">Mongo Ruby driver</a>'s <a href="https://github.com/mongodb/mongo-ruby-driver/blob/master/lib/mongo/collection.rb#L371">insert method takes a collection</a>; thus, you can insert an array of hashes quite efficiently. Even if you are using a <a href="http://mongoid.org/en/mongoid/index.html">ODM like Mongoid</a>, you can still perform batch inserts as all you need to do is get a reference to the model object's underlying collection and then issue an <code>insert</code> with an array of hashes matching the collection's intended document structure.</p>

<p>For instance, to insert a collection of <code>Tag</code> models (each having 3 fields: <code>name</code>, <code>system_tag</code>, and <code>account_id</code>) in one fell swoop I can do the following:</p>

<p><code>ruby Batch inserts with Mongoid model example
tags = ['a', 'bunch', 'of', 'tags'].collect { |tag| {name: tag, system_tag: true, account_id: id} }
Tag.collection.insert tags
</code></p>

<p>In the code above, the <code>insert</code> takes a collection of hashes; what's more, the <code>insert</code> is tied to the <code>tags</code> collection via the <code>Tag.collection</code> call.</p>

<p>Batch inserts are always faster if you have a lot of similar documents -- in our case, we saw a <em>tremendous</em> performance increase when employing batching.</p>
]]></content>
  </entry>
  
</feed>
