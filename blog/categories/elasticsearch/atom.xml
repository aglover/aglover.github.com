<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: elasticsearch | The Disco Blog]]></title>
  <link href="http://thediscoblog.com/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://thediscoblog.com/"/>
  <updated>2013-11-26T11:29:07-05:00</updated>
  <id>http://thediscoblog.com/</id>
  <author>
    <name><![CDATA[Andrew Glover]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Elasticsearch in a box]]></title>
    <link href="http://thediscoblog.com/blog/2013/11/25/elasticsearch-in-a-box/"/>
    <updated>2013-11-25T13:42:00-05:00</updated>
    <id>http://thediscoblog.com/blog/2013/11/25/elasticsearch-in-a-box</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/mine/esbox.jpg">Are you looking to get going with <a href="http://thediscoblog.com/blog/categories/elasticsearch/">Elasticsearch</a> as quickly as possible without having to worry about <a href="http://thediscoblog.com/blog/2013/05/17/elasticsearch-on-ec2-in-less-than-60-seconds/">installing Java or Elasticsearch</a> itself? Are you looking for a repeatable and automated mechanism for bringing up Elasticsearch instances for developmental and or testing purposes? While there's certainly a number of <a href="http://www.elasticsearch.org/">Elasticsearch</a>-as-a-platform service providers out there, there's one other option: use <a href="https://github.com/aglover/coffer">Elasticsearch-in-a-box</a>.</p>

<p>Elasticsearch-in-a-box is a freely available <a href="http://www.vagrantbox.es/">Vagrant base box</a>. What that means is that you can quickly fire up and tear down an Elasticsearch environment with <a href="http://docs.vagrantup.com/v2/getting-started/">simple commands</a> like <code>vagrant up</code> and <code>vagrant destroy</code>.</p>

<!-- more -->


<p>In order to use Elasticsearch-in-a-box, you first need to have <a href="http://docs.vagrantup.com/v2/installation/">Vagrant</a> and <a href="https://www.virtualbox.org/">VirtualBox</a> installed. These two installations couldn't be any easier. To install <a href="http://thediscoblog.com/blog/2013/10/16/ssh-and-vagrant/">Vagrant</a>, simply go to the downloads page and pick your target distribution. Vagrant provisions machines on top of virtual machine providers like VMWare, <a href="http://thediscoblog.com/blog/categories/aws/">AWS</a>, and VirtualBox. VirtualBox is free and easy to install -- like Vagrant, simply <a href="https://www.virtualbox.org/wiki/Downloads">go to the downloads section</a> and pick your target platform.</p>

<p>Once you have both Vagrant and VirtualBox installed, you are two steps away from Elasticsearch-ing.</p>

<p>First, you need to add and initialize the Elasticsearch-in-a-box <a href="http://docs.vagrantup.com/v2/boxes.html">template</a>. Go ahead and create a directory, like <code>/projects/esinabox</code>, change directories into it and execute this command:</p>

<p><code>bash This command will create a Vagrant definition named esinabox from the downloaded template
vagrant box add esinabox https://s3.amazonaws.com/coffers/esinabox.box
</code></p>

<p>This command will download the Elasticsearch-in-a-box template. Once that completes (it'll take a few moments depending on your connection), execute this command:</p>

<p><code>bash Vagrant init will create a VagrantFile
vagrant init 'esinabox'
</code></p>

<p>This command will create a <code>VagrantFile</code>, which you can use to customize the Elasticsearch-in-a-box instance. By default, you shouldn't need to do much, however, you can map network ports, install additional software via <a href="http://thediscoblog.com/blog/2013/11/18/provisioning-ubuntu-with-java-in-3-steps/">Bash</a>, Chef, and Puppet at your discretion.</p>

<p>Next, fire up Elasticsearch-in-a-box like so:</p>

<p><code>bash Starting up Elasticsearch-in-a-box
vagrant up
</code></p>

<p>Now that Elasticsearch-in-a-box is running locally on your machine, you can open up a new terminal and execute RESTful commands like normal because Elasticsearch is running on same ports: 9200 &amp; 9300. So go ahead and execute some queries, like so:</p>

<p><code>bash Elasticsearch is up an running!
curl -XGET 'http://localhost:9200/_status?pretty=true'
</code></p>

<p>And when you are done, go ahead and tear down the instance like so:</p>

<p><code>bash Destroying a VM instance
vagrant destroy -f
</code></p>

<p>Wasn't that easy? The Elasticsearch-in-a-box Vagrant template was built using <a href="https://github.com/jedi4ever/veewee">Veewee</a>. The base box is 64-bit <a href="http://www.ubuntu.com/index_asus.html">Ubuntu 12.04</a> with Oracle's <a href="http://thediscoblog.com/blog/2013/11/18/provisioning-ubuntu-with-java-in-3-steps/">Java 7</a> and <a href="http://www.elasticsearch.org/download/">Elasticsearch version 0.90.7</a>.</p>

<p>If you're looking for a quick and easy way to automatically provision Elasticsearch, then look no further and give <a href="https://github.com/aglover/coffer">Elasticsearch-in-a-box</a> a try!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I like my ElasticSearch a la Node.js]]></title>
    <link href="http://thediscoblog.com/blog/2013/09/21/i-like-my-elasticsearch-a-la-node-dot-js/"/>
    <updated>2013-09-21T08:57:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/09/21/i-like-my-elasticsearch-a-la-node-dot-js</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/mine/320px-Pie_A_La_Mode.jpg">While <a href="http://thediscoblog.com/blog/categories/elasticsearch/">ElasticSearch</a> is easy enough to work with via its RESTful HTTP API, there are <a href="http://www.elasticsearch.org/guide/clients/">myriad client libraries</a> available in almost every conceivable programming language. If <a href="http://thediscoblog.com/blog/categories/node/">Node.js</a> is your language of choice, then there's at least two actively supported libraries available.</p>

<p>My favorite is dubbed, albeit rather dully, "<a href="https://github.com/phillro/node-elasticsearch-client">Elastic Search Client</a>", but don't let the library's unimaginative name fool you: this is a handy library that allows you to do everything you could do via cURL with the added benefit of JavaScript callbacks. Best of all, you can use the Node Elastic Search Client in <a href="http://thediscoblog.com/blog/categories/coffeescript/">Coffeescript</a>, which is a handy language that makes JavaScript less verbose and that ultimately compiles into JavaScript.</p>

<!-- more -->


<p>Accordingly, if you're familiar with the typical RESTful API calls for creating and mapping indexes, plus indexing and searching documents, then you'll find Elastic Search Client easy enough to pick up.</p>

<p>To get started, add the library as a dependency in your NPM <code>package.json</code> file like so:</p>

<p>``` json My package.json file that lists the latest version of elasticsearchclient.
"dependencies":{</p>

<pre><code>"elasticsearchclient" : "latest",
"mocha" : "latest",
"should" : "latest",
"coffee-script" : "latest"
</code></pre>

<p>}
```</p>

<p>Since I happen to prefer <a href="http://coffeescript.org/">CoffeeScript</a> over JavaScript, I've also included CoffeeScript as a dependency.</p>

<p>Like any Node library, you'll need to include a library it via a <code>require</code> statement to make use of it. In this case, I'll require <code>'elasticsearchclient'</code> and then connect to a local instance like so:</p>

<p><code>javascript Initalizing a new client
ElasticSearchClient = require 'elasticsearchclient'
client = new ElasticSearchClient { host: 'localhost', port: 9200 }
</code></p>

<p>Going forward, I'm going to reference a few variables, namely <code>indexName</code>, which is "beer_recipes" and <code>objName</code>, which is "beer". What's more, this code is using <a href="http://visionmedia.github.io/mocha/">Mocha</a> and <a href="https://github.com/visionmedia/should.js/">should</a>, so that'll explain the various specification related statements in the examples below.</p>

<p>With a connection to an Elasticsearch server, I can consequently create an index like so:</p>

<p>``` javascript Creating an index in a before clause
describe 'Create and update an index', -></p>

<pre><code>before (done) -&gt;
    client.createIndex(indexName).on 'data', (data) -&gt;
        data = JSON.parse data
        data.should.be.ok
        done()
    .exec()
</code></pre>

<p>```</p>

<p>And I can update the index's mapping too. Just use the <code>putMapping</code> call:</p>

<p>``` javascript Updating an Index mapping
it 'should support a mapping put which changes the analyzer', (done) -></p>

<pre><code>snowball = { "mappings" : { "beer" : { "properties" : { "ingredients" : { "type" : "string", "analyzer" : "snowball" } } } }}
client.putMapping(indexName, objName, snowball).on 'data', (data) -&gt;
    data = JSON.parse data
    data.should.be.ok
    done()
.exec()
</code></pre>

<p>```</p>

<p>In this case, the actual mapping JSON document is identical to what I'd have to pass via cURL, for instance.</p>

<p>You should start to notice a pattern with respect to how the Elastic Search Client deals with callbacks -- using an <code>on</code> method, you can register a callback for <code>'data'</code>, which essentially entails the response from the server; moreover, you can also register a listener for <code>'error'</code>, which as you can imagine, gets invoked if there is a problem.</p>

<p>Deleting an index is just as easy as creating one too:</p>

<p>``` javascript Deleting an index
after (done) -></p>

<pre><code>client.deleteIndex(indexName).on 'data', (data) -&gt;
    data = JSON.parse data
    data.should.be.ok
    done()
.exec()
</code></pre>

<p>```</p>

<p>To index a document, you simply use the <code>index</code> function like so:</p>

<p>``` javascript Indexing a document
beer = { "name": "Todd Enders' Witbier", "style": "wit, Belgian ale, wheat beer", "ingredients": "4.0 ..."}
client.index(indexName, objName, beer).on 'data', (data) -></p>

<pre><code>data = JSON.parse data
data.ok.should.equal true
</code></pre>

<p>.exec()
```</p>

<p>Note in this case, I pass along an index name, and object name, and a JSON document to index. Because JSON marries so nicely with JavaScript, this library is quite nice, don't you think?</p>

<p>Searching is just as easy (as I'm sure you already guessed). You create a query JSON document and pass it along to the <code>search</code> method like so:</p>

<p>``` javascript Searching is just as easy!
iquery = { "query" : { "term" : { "ingredients" : "lemons" } } }
client.search(indexName, objName, iquery).on 'data', (idata) -></p>

<pre><code>result = JSON.parse idata
result.hits.total.should.equal 1
done()
</code></pre>

<p>.exec()
```</p>

<p>The resultant response from the server is a JSON document that must be parsed and dealt with accordingly.</p>

<p>Of course, you can grab an individual document via its id just as easily:</p>

<p>``` javascript Getting an individual document via its document id
client.get(indexName, objName, doc_id).on 'data', (getData) -></p>

<pre><code>doc = JSON.parse getData
doc._source.name.should.equal "Todd Enders' Witbier"
done()
</code></pre>

<p>.exec()
```</p>

<p>Node's Elastic Search Client is a breeze to pick up; plus, the natural union of JSON and JavaScript make Node a perfect language for interfacing with ElasticSearch. Dig it?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding ElasticSearch analyzers]]></title>
    <link href="http://thediscoblog.com/blog/2013/09/14/understanding-elasticsearch-analyzers/"/>
    <updated>2013-09-14T08:37:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/09/14/understanding-elasticsearch-analyzers</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/mine/old-beer.jpg">Sadly, lots of early Internet beer recipes aren't necessarily in an easily digestible format; that is, these recipes are <em>unstructured</em> intermixed lists of directions and ingredients often originally composed in an email or forum post.</p>

<p>So while it's hard to easily put these recipes into traditional data stores (ostensibly for easier searching), they're perfect for <a href="http://thediscoblog.com/blog/2013/05/14/the-democratization-of-search/">ElasticSearch</a> in their current form.</p>

<p>Accordingly, imagine an <a href="http://thediscoblog.com/blog/2013/05/17/elasticsearch-on-ec2-in-less-than-60-seconds/">ElasticSearch</a> index full of beer recipes, since...well...I enjoy making beer (and drinking it too).</p>

<!-- more -->


<p>First, I'll add some beer recipes into ElasticSearch using <a href="https://github.com/phillro/node-elasticsearch-client">Node's ElasticSearch Client</a>(note that the code is <a href="http://thediscoblog.com/blog/categories/coffeescript/">CoffeeScript</a> though). I'll be adding these beer recipes into a <code>beer_recipes</code> index like so:</p>

<p>``` javascript Adding a beer recipe
beer_1 = {</p>

<pre><code>name: "Todd Enders' Witbier",
style: "wit, Belgian ale, wheat beer",
ingredients: "4.0 lbs Belgian pils malt, 4.0 lbs raw soft red winter wheat, 0.5 lbs rolled oats, 0.75 oz coriander, freshly ground Zest from two table oranges and two lemons, 1.0 oz 3.1% AA Saaz, 3/4 corn sugar for priming, Hoegaarden strain yeast"
</code></pre>

<p>}</p>

<p>client.index('beer_recipes', 'beer', beer_1).on('data', (data) -></p>

<pre><code>console.log(data)
</code></pre>

<p>).exec()
```</p>

<p>Note how the interesting part of a recipe JSON document, dubbed <code>beer_1</code> is found in the <code>ingredients</code> field. This field is basically a big string of valuable text (you can imagine how this string was essentially the body of an email). So while the <code>ingredients</code> field is unstructured, it's something clearly that people will want to search on.</p>

<p>I will add one more recipe for good measure:</p>

<p>``` javascript Adding a second beer recipe
beer_2 = {</p>

<pre><code>name: "Wit",
style: "wit, Belgian ale, wheat beer",
ingredients: "4 lbs DeWulf-Cosyns 'Pils' malt, 3 lbs brewers' flaked wheat (inauthentic; will try raw wheat nest time), 6 oz rolled oats, 1 oz Saaz hops (3.3% AA), 0.75 oz bitter (Curacao) orange peel quarters (dried), 1 oz sweet orange peel (dried), 0.75 oz coriander (cracked), 0.75 oz anise seed, one small pinch cumin, 0.75 cup corn sugar (priming), 10 ml 88% food-grade lactic acid (at bottling), BrewTek 'Belgian Wheat' yeast"
</code></pre>

<p>}</p>

<p>client.index('beer_recipes', 'beer', beer_2).on('data', (data) -></p>

<pre><code>console.log(data)
</code></pre>

<p>).exec()
```</p>

<p>It's a hot summers day and I'm thinking I'd like to make a beer with <em>lemon</em> as an ingredient (to be clear: I want to use lemon zest, which is obtained from a lemon peel). So naturally, I need to find (i.e. <em>search for</em>) a recipe with lemons in it.</p>

<p>Consequently, I'll search my index for recipes that contain the word "lemon" like so:</p>

<p>``` javascript Searching for lemon
query = { "query" : { "term" : { "ingredients" : "lemon" } } }</p>

<p>client.search('beer_recipes', 'beer', query).on('data', (data) -></p>

<pre><code>data = JSON.parse(data)
for doc in data.hits.hits
    console.log doc._source.style
    console.log doc._source.name
    console.log doc._source.ingredients
</code></pre>

<p>).exec()
```</p>

<p>But nothing shows up -- there are no results! Why is that?</p>

<p>If you look closely in the earlier code example (specifically, the <code>beer_1</code> JSON document), you can see that the word "lemons" is in the text (i.e. "...two table oranges and two lemons..."). It turns out, by default, the way values are indexed by ElasticSearch, <em>lemon</em> doesn't necessarily match -- <em>lemons</em> does though.</p>

<p>``` javascript Searching for lemons
query = { "query" : { "term" : { "ingredients" : "lemons" } } }</p>

<p>client.search('beer_recipes', 'beer', query).on('data', (data) -></p>

<pre><code>data = JSON.parse(data)
for doc in data.hits.hits
    console.log doc._source.style
    console.log doc._source.name
    console.log doc._source.ingredients
</code></pre>

<p>).exec()
```</p>

<p>Lo and behold, this search returns a hit! But that's inconvenient, to say the least. Basically the words in the <code>ingredients</code> field are tokenized <em>as is</em>. Hence, a search for "lemons" works while "lemon" doesn't. Note: there are various mechanisms for searching, and a search on "lemon*" should have returned a result.</p>

<p>When a document is added into an <a href="http://thediscoblog.com/blog/2013/01/02/scalable-searching-with-elasticsearch/">ElasticSearch</a> index, its fields are analyzed and converted into <em>tokens</em>. When you execute a search against an index, you search against those tokens. How ElasticSearch tokenizes a document is configurable.</p>

<p>There are different ElasticSearch analyzers available -- from language analyzers that allow you to support non-English language searches to the <a href="http://snowball.tartarus.org/">snowball</a> analyzer, which converts a word into its root (or stem and that process of creating a stem from a word is called <em><a href="http://snowball.tartarus.org/algorithms/english/stemmer.html">stemming</a></em>), yielding a simpler token. For example, a snowball of "lemons" would be "lemon". Or if the words "knocks" and "knocking" were in a snowball analyzed document, both terms would have "knock" as a stem.</p>

<p>You can change how documents are tokenized via the index mapping API like so:</p>

<p>``` bash Changing the mapping for an index using cURL
curl -XPUT 'http://localhost:9200/beer_recipes' -d '{ "mappings" : {
  "beer" : {</p>

<pre><code>"properties" : {
  "ingredients" : { "type" : "string", "analyzer" : "snowball" }
}
</code></pre>

<p>   }
 }
}'
```</p>

<p>Note how the above mapping specifies that the <code>ingredients</code> field will be analyzed via the snowball analyzer. Also note, you have to change the mapping of an index <em>before</em> you begin to add documents to it! So, in this case, I'll need to drop the index, run the mapping call above, and then re-add those two recipes.</p>

<p>Now I can begin searching recipes for the ingredient "lemon"  or "lemons".</p>

<p>``` javascript Searching for lemon now works!
query = { "query" : { "term" : { "ingredients" : "lemon" } } }</p>

<p>client.search('beer_recipes', 'beer', query).on('data', (data) -></p>

<pre><code>data = JSON.parse(data)
for doc in data.hits.hits
    console.log doc._source.style
    console.log doc._source.name
    console.log doc._source.ingredients
</code></pre>

<p>).exec()
```</p>

<p>Keep in mind that <a href="http://stackoverflow.com/questions/3875382/lucene-standard-analyzer-vs-snowball">snowballing can inadvertently</a> make your search results <em>less relevant</em>. Long words can be stemmed into more common but completely different words. For example, if you snowball a document that contains the word "sextant", the word "sex" will result as a stem. Thus, searches for "sextant" will also return documents that contain the word "sex" (and vice versa).</p>

<p>ElasticSearch puts a powerful search engine into your clutches; plus, with a little forethought into how a document's contents are analyzed, you'll make searches event more relevant.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Effortless ElasticSearch clustering]]></title>
    <link href="http://thediscoblog.com/blog/2013/09/03/effortless-elasticsearch-clustering/"/>
    <updated>2013-09-03T07:50:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/09/03/effortless-elasticsearch-clustering</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/mine/cluster.jpg"><a href="http://thediscoblog.com/blog/categories/elasticsearch/">ElasticSearch</a> supports <a href="http://www.elasticsearch.org/guide/reference/modules/cluster/">clustering</a>; that is, you can have a series of distinct <a href="http://thediscoblog.com/blog/2013/05/14/the-democratization-of-search/">ElasticSearch</a> instances work in a coordinated manner without much administrative intervention at all. Clustering ElasticSearch instances (or nodes) provides data redundancy as well as data availability.</p>

<p>Best of all, clustering in ElasticSearch, by default, doesn't require <em>any</em> configuration -- nodes discover each other. You can set up a cluster in about 60 seconds. Let me show you how!</p>

<!-- more -->


<p></p>

<p>First, download and unzip or untar the <a href="http://thediscoblog.com/blog/2013/01/02/scalable-searching-with-elasticsearch/">latest version of ElasticSearch</a>. Next, copy the resultant ElasticSearch install directory (for example, mine is dubbed <code>elasticsearch-0.90.3</code>) into 3 different directories; for example, I've called my directories <code>node-1</code>, <code>node-2</code>, and <code>node-3</code>.</p>

<p>Next, open up three terminal windows and in each, change directories to a sequential node. Start the instance in the <code>node-1</code> directory like so:</p>

<p><code>bash Starting up ElasticSearch
./bin/elasticsearch -f
</code></p>

<p>The <code>-f</code> forces the process to run in the foreground.</p>

<p>By default, ElasticSearch nodes will name themselves if you don't provide a name (via the <code>elasticsearch.yml</code> configuration file). Thus, after you start the first instance, you should see something along the lines of:</p>

<p><code>bash A master node is created!
[cluster.service] [Dionysus] new_master [Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
</code></p>

<p>In the above log output, "Dionysus" is the automatic name chosen by ElasticSearch. Note the part about "new_master" for the <code>cluster.service</code>.</p>

<p>Next, go into the next terminal window, say <code>node-2</code>, and start that instance the same way (via the <code>-f</code> flag). You should see the 2nd instance (in my case, named <code>Caiera</code>) discover the master:</p>

<p><code>bash Node #2 discovers the master node
[cluster.service] [Caiera] detected_master [Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]], added {[Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]],}, reason: zen-disco-receive(from master [[Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]]])
</code></p>

<p>See the <code>detected_master</code> statement above?  And you should see the master, "Dionysus", add the 2nd instance, "Caiera", too (via the first terminal window):</p>

<p><code>bash The master node adds node #2
[cluster.service] [Dionysus] added {[Caiera][Eh3DHlcRQhGxatGnUG8smA][inet[/192.168.1.12:9301]],}, reason: zen-disco-receive(join from node[[Caiera][Eh3DHlcRQhGxatGnUG8smA][inet[/192.168.1.12:9301]]])
</code></p>

<p>Lastly, start the 3rd instance in the 3rd window.</p>

<p>In this case, my 3rd node is dubbed "Phantom Rider" and it'll discover the master:</p>

<p><code>bash Node #3 discovers the master
[cluster.service] [Phantom Rider] detected_master [Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]], added {[Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]],[Caiera][Eh3DHlcRQhGxatGnUG8smA][inet[/192.168.1.12:9301]],}, reason: zen-disco-receive(from master [[Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]]])
</code></p>

<p>And the master, will in turn, add "Phantom Rider" into the cluster:</p>

<p><code>bash The master node adds node #3
[cluster.service] [Dionysus] added {[Phantom Rider][Sw1bDSbFQqeTq4M45qbNpg][inet[/192.168.1.12:9302]],}, reason: zen-disco-receive(join from node[[Phantom Rider][Sw1bDSbFQqeTq4M45qbNpg][inet[/192.168.1.12:9302]]])
</code></p>

<p>Now that you have 3 ElasticSearch instances running, you can run a few RESTful commands to verify your cluster is operational.</p>

<p>First, in a new terminal window, run this command:</p>

<p><code>bash cURL to ascertain cluster nodes
curl -XGET 'http://localhost:9200/_cluster/nodes?pretty=true'
</code></p>

<p>You should see a nicely formatting JSON response. It'll look something like:</p>

<p>``` json ElasticSearch nodes
{
  "cluster_name" : "elasticsearch",
  "nodes" : { "Eh3DHlcRQhGxatGnUG8smA" : { "hostname" : "new-host-5.home",</p>

<pre><code>      "http_address" : "inet[/192.168.1.12:9201]",
      "name" : "Caiera",
      "transport_address" : "inet[/192.168.1.12:9301]",
      "version" : "0.90.3"
    },
  "Sw1bDSbFQqeTq4M45qbNpg" : { "hostname" : "new-host-5.home",
      "http_address" : "inet[/192.168.1.12:9202]",
      "name" : "Phantom Rider",
      "transport_address" : "inet[/192.168.1.12:9302]",
      "version" : "0.90.3"
    },
  "r7gbosdKSWGfTCgRPrS6vw" : { "hostname" : "new-host-5.home",
      "http_address" : "inet[/192.168.1.12:9200]",
      "name" : "Dionysus",
      "transport_address" : "inet[/192.168.1.12:9300]",
      "version" : "0.90.3"
    }
},
</code></pre>

<p>  "ok" : true
}
```</p>

<p>Note the <code>cluster_name</code> is by default elasticsearch -- you can change this name as well. Also, by default, the master node will claim the 9200 port, however, you can run that same command against any node (for example, <code>http://localhost:9201/_cluster/nodes?pretty=true</code> will respond with the same exact response).</p>

<p>You can check the health of a cluster, as well, by running this command:</p>

<p><code>bash cURL to ascertain cluster health
curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'
</code></p>

<p>And you should see another JSON response like:</p>

<p><code>json ElasticSearch cluster health
{
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "cluster_name" : "elasticsearch",
  "initializing_shards" : 0,
  "number_of_data_nodes" : 3,
  "number_of_nodes" : 3,
  "relocating_shards" : 0,
  "status" : "green",
  "timed_out" : false,
  "unassigned_shards" : 0
}
</code></p>

<p>Nodes a cluster will elect a new master if your master node goes down. For example, if you go into the master node terminal window and control-c the process, you should see the two other nodes quickly recognize the master's failure and consequently elect one of themselves as the new master:</p>

<p><code>bash A new master is elected
[discovery.zen] [Caiera] master_left [[Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]]], reason [shut_down]
[cluster.service] [Caiera] master {new [Caiera][Eh3DHlcRQhGxatGnUG8smA][inet[/192.168.1.12:9301]], previous [Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]]}, removed {[Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]],}, reason: zen-disco-master_failed ([Dionysus][r7gbosdKSWGfTCgRPrS6vw][inet[/192.168.1.12:9300]])
</code></p>

<p>Remember, ElasticSearch will make intelligent defaults for you, however, you should most likely look closely at the various aspects you can configure when it comes to clustering. For example, cluster name and node names are something you should consider implementing.</p>

<p>What's more, the process of discovery can be configured. For instance, multicast is used for auto-discovery, however, there are other options available, such as unicast, which allows you to specify which nodes can be a part of a cluster (that is, unknown nodes cannot join).</p>

<p>Finally, you can control how nodes operate in a cluster. You can explicitly forbid a node from being a master, for example, and you can also configure nodes to not hold data (thus, those nodes become veritable routers for searches). These options allow you to create an interesting topology that has some intelligent routing built in.</p>

<p>Regardless of how you configure an ElasticSearch cluster, I hope you've seen that it couldn't be any easier. Can you dig it, man?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ElasticSearch on EC2 in less than 60 seconds]]></title>
    <link href="http://thediscoblog.com/blog/2013/05/17/elasticsearch-on-ec2-in-less-than-60-seconds/"/>
    <updated>2013-05-17T17:24:00-04:00</updated>
    <id>http://thediscoblog.com/blog/2013/05/17/elasticsearch-on-ec2-in-less-than-60-seconds</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/mine/es-bonzai.jpg">Curious to see what all the <a href="http://www.elasticsearch.org/">ElasticSearch</a> hubbub is about? Wanna see it in action without a lot of elbow grease? Then look no further, friend -- in less than 60 seconds, I'll show you how to install <a href="http://www.ibm.com/developerworks/java/library/j-javadev2-24/">ElasticSearch</a> on an <a href="http://aws.amazon.com/">AWS AMI</a>.</p>

<p>You'll first <a href="http://www.drdobbs.com/web-development/getting-started-with-the-cloud-amazon-we/231601598">need an AWS account</a> along with an SSH key pair. If you don't already have those two steps done, go ahead and do that. The steps that follow suggest a particular AMI; however, you are free to select the <a href="http://aws.amazon.com/ec2/instance-types/">instance type</a>. <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html">Micro instance types</a> are free to use; consequently, you can get <a href="http://thediscoblog.com/blog/2013/05/14/the-democratization-of-search/">up and running with ElasticSearch</a> in less than a minute <em>for free</em>.</p>

<!-- more -->


<p>Now that you've got an <a href="http://www.ibm.com/developerworks/web/library/j-s3/">AWS</a> account and an SSH key pair, go ahead and create a new security group (or edit an existing one). It's important that the <a href="http://www.elasticsearch.org/tutorials/elasticsearch-on-ec2/">following ports</a> are open:</p>

<ul>
<li>22 (required for SSH)</li>
<li>80 (ElasticSearch uses HTTP for standard API calls)</li>
<li>9200 (required for ElasticSearch)</li>
<li>9300 (required for ElasticSearch)</li>
</ul>


<p>Next, fire up a Linux AMI. I, for example, prefer <a href="http://cloud-images.ubuntu.com/locator/ec2/">ami-c30360aa</a> (this is Ubuntu Server version 13.04) and I configure the AMI to use the security group that I just covered.</p>

<p>Now, SSH to your newly instantiated AMI.  Once on the AMI, you'll need to install Java. Never fear though, I've got you covered. All you need to do is run a handy script via the <a href="https://github.com/aglover/ubuntu-equip">Ubuntu-Equip project</a>, that I use frequently just for this sorta thing:</p>

<p><code>bash installing Java
wget --no-check-certificate https://github.com/aglover/ubuntu-equip/raw/master/equip_java.sh &amp;&amp; bash equip_java.sh
</code></p>

<p>You'll need to accept the license from Oracle. Once that script completes, go ahead and type  <code>java -version</code> and you should see Oracle's JDK (i.e Java version "1.7.0_21").</p>

<p>Next, download and install ElasticSearch via another nifty <a href="https://github.com/aglover/ubuntu-equip">Ubuntu-Equip</a> script:</p>

<p><code>bash installing elasticsearch
wget --no-check-certificate https://github.com/aglover/ubuntu-equip/raw/master/equip_elasticsearch.sh &amp;&amp; bash equip_elasticsearch.sh
</code></p>

<p>This script doesn't start ElasticSearch for you; thus, go ahead and change directories into the <code>elasticsearch</code> directory and fire it up like so:</p>

<p><code>bash starting elasticsearch
~/elasticsearch$ bin/elasticsearch -f
</code></p>

<p>Take a deep breath (but not too deep, as I need you to finish in less than 60 seconds) and find the Public DNS of the AMI you've been working on. Go ahead and copy it, then fire up a browser on your local machine and go to http://YOUR_AMI_DNS_NAME.com:9200/_plugin/inquisitor/ (be sure to note the port).</p>

<p>By the way, <a href="https://github.com/polyfractal/elasticsearch-inquisitor">Inquisitor</a> is a handy web application that lets you query your indexes. It was installed via the Ubuntu-Equip script -- this tool is invaluable in figuring out how to properly query your indexes.</p>

<p>And that is it. In less than 60 seconds you've got ElasticSearch running in the cloud for you. Want to create a cluster? No problem, just follow these steps again to fire up another ElasticSearch instance and then <a href="http://www.elasticsearch.org/videos/three-nodes-and-one-cluster/">configure the cluster accordingly</a>.</p>

<p>I've not gone over <a href="http://www.elasticsearch.org/guide/reference/setup/configuration/">configuring ElasticSearch</a> nor have I showed you how to create ElasticSearch as a service on a Linux instance, but for one minute, what do you expect?</p>
]]></content>
  </entry>
  
</feed>
